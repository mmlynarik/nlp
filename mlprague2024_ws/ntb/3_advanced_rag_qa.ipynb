{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06ca8260-b16d-41fc-9e29-28baf966a95d",
   "metadata": {},
   "source": [
    "### Install necessary dependencies\n",
    "If you need to uninstall the dependencies, you can run e.g. `!pip uninstall -r ../requirements.txt -y` in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4cf5b2ed-8883-42ed-8a99-1a5227bfd713",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m pip install -r ../requirements.txt --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03cd97d7-bf64-4c9b-8a71-ba7079f34538",
   "metadata": {},
   "source": [
    "### Make Python scripts accessible\n",
    "There is a couple of Python scripts in the `/src` directory. We can make them accessible by adding the directory to the path ENV variable. We insert the path to position 1 to make it the first path scanned for the required modules to not confuse our scripts with scripts with the same name but in unrelated locations. This change to ENV is temporary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "446ff7e7-293b-4963-bfe2-ba4986742894",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "path_to_src = os.path.abspath('../src')\n",
    "\n",
    "if path_to_src not in sys.path:\n",
    "    sys.path.insert(1, path_to_src)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea190c7-1876-4448-90bf-393a40809146",
   "metadata": {},
   "source": [
    "### Import dependencies\n",
    "\n",
    "Just common dependencies except the aws_cli - that is our own script for accessing Amazon Bedrock service. The Bedrock is a home of the models we are going to use for text embedding and text generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c4b4571-395d-42ea-9c25-2683520b35c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from rank_bm25 import BM25Okapi\n",
    "import re\n",
    "import string\n",
    "import textwrap\n",
    "\n",
    "from aws_cli import Client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62a0447d-ba45-4636-8f27-3d04869dcc30",
   "metadata": {},
   "source": [
    "### Basic RAG QA\n",
    "The final solution from the previous notebook has been just copy-pasted here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d1c33078-dbce-459e-b90d-151c2a6a85b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicRAGQA:\n",
    "    def __init__(self, chunk_size=250, chunk_overlap=50, k=8):\n",
    "        self.client = Client()\n",
    "        \n",
    "        # ensure the chunking settings make sense\n",
    "        chunk_size = abs(chunk_size)\n",
    "        chunk_overlap = abs(chunk_overlap)\n",
    "\n",
    "        if (chunk_size - chunk_overlap) < 1:\n",
    "            raise Exception('The chunk_size needs to be larger than chunk_overlap')\n",
    "\n",
    "        k = abs(k)\n",
    "\n",
    "        if not 0 < k <= 20:\n",
    "            raise Exception('The k needs to be between 1 and 20')\n",
    "\n",
    "        self.chunk_size = chunk_size\n",
    "        self.chunk_overlap = chunk_overlap\n",
    "        self.k = k\n",
    "\n",
    "\n",
    "    def load_knowledge_base(self, dir_path):\n",
    "        # read all documents\n",
    "        documents_list = []\n",
    "        for file_name in os.listdir(dir_path):\n",
    "            if file_name.endswith('.json'):\n",
    "                documents_list.append(pd.read_json(os.path.join(dir_path, file_name)))\n",
    "    \n",
    "        # build database\n",
    "        documents = pd.concat(documents_list, ignore_index=True)\n",
    "    \n",
    "        # split documents into chunks\n",
    "        documents['chunks'] = documents['content'].apply(self._str2chunks)\n",
    "        documents = documents.explode('chunks').reset_index(drop=True)\n",
    "        documents = documents.drop(columns=['content'])\n",
    "        documents = documents.rename(columns={'chunks': 'content'})\n",
    "    \n",
    "        # build index\n",
    "        embeddings = documents['content'].apply(self.client.embed_text)\n",
    "    \n",
    "        # store knowledge base\n",
    "        self.knowledge_base = {'documents': documents, 'index': np.stack(embeddings, axis=0)}\n",
    "\n",
    "    def _str2chunks(self, text):    \n",
    "        return [text[a:a + self.chunk_size] for a in range(0, len(text), self.chunk_size - self.chunk_overlap)]\n",
    "\n",
    "    def _retriever(self, query):\n",
    "        # embed user query\n",
    "        query_embedding = self.client.embed_text(query)  # use the same embedding that was used for the knowledge base\n",
    "\n",
    "        # retrieve most similar document\n",
    "        similarities = np.dot(self.knowledge_base['index'], query_embedding)\n",
    "        top_k_idxs = np.argpartition(similarities, -self.k)[-self.k:]\n",
    "        top_k_sorted_idxs = top_k_idxs[np.argsort(similarities[top_k_idxs])][::-1]\n",
    "        top_k_documents = self.knowledge_base['documents'].iloc[top_k_sorted_idxs]\n",
    "    \n",
    "        return top_k_documents, similarities[top_k_sorted_idxs]\n",
    "\n",
    "    def _construct_prompt(self, query_text, documents_text):\n",
    "        prompt = textwrap.dedent(\n",
    "            '''\\\n",
    "            <s>[INST]Use only the below-given KNOWLEDGE and not prior knowledge to provide an accurate, helpful, concise, and clear answer to the QUERY below.\n",
    "            Avoid copying word-for-word from the KNOWLEDGE and try to use your own words when possible.\n",
    "    \n",
    "            KNOWLEDGE:\n",
    "            {texts}\n",
    "    \n",
    "            Answer the QUERY using only the provided KNOWLEDGE. Don't provide notes, comments, or explanations.\n",
    "            After the answer, write a paragraph starting with \"References: \" followed by the [id] of each reference article containing information needed to answer the query.\n",
    "            If none of the articles from KNOWLEDGE contains information needed to provide a precise answer, or if you are not 100 % sure, reply with string: \"DONT_KNOW\".\n",
    "\n",
    "            QUERY: \"{query_text}\"\n",
    "            ANSWER:[/INST]\n",
    "            '''\n",
    "        ).format(\n",
    "            query_text = query_text,\n",
    "            texts = '\\n\\n'.join(['Article [{}]: \"\"\"\\n{}\\n\"\"\"'.format(idx, text) for idx, text in documents_text.reset_index(drop=True).items()])\n",
    "        )\n",
    "    \n",
    "        return prompt\n",
    "\n",
    "    def process_query(self, query):\n",
    "        documents, similarities = self._retriever(query)\n",
    "        prompt = self._construct_prompt(query, documents.content)\n",
    "        answer = self.client.execute_prompt(prompt)\n",
    "\n",
    "        answer_and_refs = answer.split('References:')\n",
    "        answer = answer_and_refs[0].strip()\n",
    "\n",
    "        llm_references = []\n",
    "        if len(answer_and_refs) > 1:\n",
    "            for ref_id in re.findall(r'\\[(\\d+)\\]', answer_and_refs[1]):\n",
    "                doc = documents.iloc[int(ref_id)]\n",
    "                llm_references.append((doc.title, doc.url, similarities[int(ref_id)]))\n",
    "\n",
    "            llm_references = sorted(llm_references, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        if ('DONT_KNOW' in answer) or (len(llm_references) == 0):\n",
    "            answer = 'I\\'m sorry, I don\\'t know answer to your query.'\n",
    "            llm_references = []\n",
    "\n",
    "        return answer, llm_references"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "219ab93c",
   "metadata": {},
   "source": [
    "Create the basic RAG, load the Wikipedia knowledge base, and test it on a couple of examples from the previous notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e72f13bd-0700-42ff-a30b-d0bc6ebc96d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG: \"In 2016, Socialbakers had 350 employees.\"\n",
      "     [ref: [('Emplifi', 'https://en.wikipedia.org/wiki/Emplifi', 126.11330250122865), ('Emplifi', 'https://en.wikipedia.org/wiki/Emplifi', 82.78630937431362)]]\n",
      "RAG: \"Jan Rus is a Research Team Lead who is currently working at Emplifi.\"\n",
      "     [ref: [('Emplifi', 'https://en.wikipedia.org/wiki/Emplifi', 99.09258361662546)]]\n",
      "RAG: \"Jan Rus is currently working at Emplifi.\"\n",
      "     [ref: [('Emplifi', 'https://en.wikipedia.org/wiki/Emplifi', 86.93297576438992), ('Emplifi', 'https://en.wikipedia.org/wiki/Emplifi', 86.93297576438992)]]\n",
      "RAG: \"I'm sorry, I don't know answer to your query.\"\n",
      "     [ref: []]\n",
      "RAG: \"I'm sorry, I don't know answer to your query.\"\n",
      "     [ref: []]\n",
      "RAG: \"I'm sorry, I don't know answer to your query.\"\n",
      "     [ref: []]\n",
      "RAG: \"Yes, Russia is bigger than Belarus.\"\n",
      "     [ref: [('Belarus', 'https://en.wikipedia.org/wiki/Belarus', 205.95386592863792), ('Belarus', 'https://en.wikipedia.org/wiki/Belarus', 198.6775665542217)]]\n",
      "RAG: \"Based on the provided knowledge, Russia is bigger than Belarus.\"\n",
      "     [ref: [('Belarus', 'https://en.wikipedia.org/wiki/Belarus', 187.64516225245112), ('Belarus', 'https://en.wikipedia.org/wiki/Belarus', 185.5182271467039)]]\n"
     ]
    }
   ],
   "source": [
    "basic_rag = BasicRAGQA()\n",
    "basic_rag.load_knowledge_base('../data/wikipedia_kb/')\n",
    "\n",
    "query1 = 'How many employees did Socialbakers have in 2016?'\n",
    "query2 = 'Who is Jan Rus?'\n",
    "query3 = 'Where is Jan Rus working now?'\n",
    "query4 = 'What is a dog?'\n",
    "query5 = 'Do dogs love social media?'\n",
    "query6 = 'Hi!'\n",
    "query7 = 'Is Russia bigger than Belarus?'  # Elaborate\n",
    "query8 = 'Which country is bigger, Russia or Belarus?'  # Elaborate\n",
    "\n",
    "answer1, reference1 = basic_rag.process_query(query1)\n",
    "answer2, reference2 = basic_rag.process_query(query2)\n",
    "answer3, reference3 = basic_rag.process_query(query3)\n",
    "answer4, reference4 = basic_rag.process_query(query4)\n",
    "answer5, reference5 = basic_rag.process_query(query5)\n",
    "answer6, reference6 = basic_rag.process_query(query6)\n",
    "answer7, reference7 = basic_rag.process_query(query7)\n",
    "answer8, reference8 = basic_rag.process_query(query8)\n",
    "\n",
    "print(f'RAG: \"{answer1}\"\\n     [ref: {reference1}]')\n",
    "print(f'RAG: \"{answer2}\"\\n     [ref: {reference2}]')\n",
    "print(f'RAG: \"{answer3}\"\\n     [ref: {reference3}]')\n",
    "print(f'RAG: \"{answer4}\"\\n     [ref: {reference4}]')\n",
    "print(f'RAG: \"{answer5}\"\\n     [ref: {reference5}]')\n",
    "print(f'RAG: \"{answer6}\"\\n     [ref: {reference6}]')\n",
    "print(f'RAG: \"{answer7}\"\\n     [ref: {reference7}]')\n",
    "print(f'RAG: \"{answer8}\"\\n     [ref: {reference8}]')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab4b056-f43c-4a2c-bbfc-4844f45a9acf",
   "metadata": {},
   "source": [
    "### Advanced RAG QA\n",
    "Let's see if a very basic hybrid search will help with the problematic cases and not break anything else."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "85645e31-ad2f-4690-a8ae-cccbff4852fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdvancedRAGQA:\n",
    "    stopwords = [\n",
    "        'actually', 'almost', 'already', 'also', 'although', 'among', 'around', 'behind', 'beside', 'besides', 'beyond', 'can', 'done',\n",
    "        'else', 'even', 'ever', 'get', 'go', 'got', 'however', 'just', 'may', 'might', 'must', 'need', 'now', 'one', 'otherwise',\n",
    "        'per', 'rather', 'really', 'since', 'still', 'than', 'till', 'us', 'via', 'whether', 'will', 'within', 'without', 'yet',\n",
    "        'aren', 'couldn', 'didn', 'doesn', 'don', 'hadn', 'hasn', 'haven', 'isn', 'mustn', 'needn', 'shouldn', 'wasn', 'weren', 'won', 'wouldn',\n",
    "        'a', 'the', 'and', 'i', 'you', 'is', 'are', 'not', 'was', 'were', 'no', 'me'\n",
    "    ]\n",
    "\n",
    "    sparse_search_min_similarity = 0.01\n",
    "    dense_search_min_similarity = 30.0\n",
    "\n",
    "    def __init__(self, chunk_size=250, chunk_overlap=50, k=8, alfa=0.5):\n",
    "        self.client = Client()\n",
    "        \n",
    "        # ensure the chunking settings make sense\n",
    "        chunk_size = abs(chunk_size)\n",
    "        chunk_overlap = abs(chunk_overlap)\n",
    "\n",
    "        if (chunk_size - chunk_overlap) < 1:\n",
    "            raise Exception('The chunk_size needs to be larger than chunk_overlap')\n",
    "\n",
    "        k = abs(k)\n",
    "\n",
    "        if not 0 < k <= 20:\n",
    "            raise Exception('The k needs to be between 1 and 20')\n",
    "\n",
    "        alfa = abs(alfa)\n",
    "\n",
    "        if not 0.0 <= alfa <= 1.0:\n",
    "            raise Exception('The alfa needs to be between 0.0 and 1.0')\n",
    "\n",
    "        self.chunk_size = chunk_size\n",
    "        self.chunk_overlap = chunk_overlap\n",
    "        self.k = k\n",
    "        self.alfa = alfa\n",
    "\n",
    "    def _tokenize_text(self, text):\n",
    "        clean_txt = text.translate(str.maketrans('', '', string.punctuation))\n",
    "        clean_txt = re.sub(r'\\n', ' ', clean_txt)\n",
    "        clean_txt = re.sub(r'\\s+', ' ', clean_txt)\n",
    "        clean_txt = clean_txt.strip()\n",
    "        \n",
    "        tokens = clean_txt.lower().split(' ')\n",
    "        tokens = [token for token in tokens if (token not in set(self.stopwords)) and (len(token)>2)]\n",
    "    \n",
    "        return tokens\n",
    "\n",
    "    def load_knowledge_base(self, dir_path):\n",
    "        # read all documents\n",
    "        documents_list = []\n",
    "        for file_name in os.listdir(dir_path):\n",
    "            if file_name.endswith('.json'):\n",
    "                documents_list.append(pd.read_json(os.path.join(dir_path, file_name)))\n",
    "    \n",
    "        # build database\n",
    "        documents = pd.concat(documents_list, ignore_index=True)\n",
    "    \n",
    "        # split documents into chunks\n",
    "        documents['chunks'] = documents['content'].apply(self._str2chunks)\n",
    "        documents = documents.explode('chunks').reset_index(drop=True)\n",
    "        documents = documents.drop(columns=['content'])\n",
    "        documents = documents.rename(columns={'chunks': 'content'})\n",
    "    \n",
    "        # build dense index\n",
    "        embeddings = documents['content'].apply(self.client.embed_text)\n",
    "\n",
    "        # build sparse index\n",
    "        corpus = list(documents['content'].values)\n",
    "        tokenized_corpus = [self._tokenize_text(document) for document in corpus]\n",
    "        bm25 = BM25Okapi(tokenized_corpus)\n",
    "    \n",
    "        # store knowledge base\n",
    "        self.knowledge_base = {\n",
    "            'documents': documents,\n",
    "            'index_dense': np.stack(embeddings, axis=0),\n",
    "            'index_sparse': bm25\n",
    "        }\n",
    "\n",
    "    def _str2chunks(self, text):    \n",
    "        return [text[a:a + self.chunk_size] for a in range(0, len(text), self.chunk_size - self.chunk_overlap)]\n",
    "\n",
    "    def _retriever(self, query):\n",
    "        # embed user query for dense search\n",
    "        query_embedding = self.client.embed_text(query)  # use the same embedding that was used for the knowledge base\n",
    "\n",
    "        # tokenize query for sparse search\n",
    "        query_tokens = self._tokenize_text(query)\n",
    "\n",
    "        # dense: retrieve most similar document\n",
    "        similarities = np.dot(self.knowledge_base['index_dense'], query_embedding)\n",
    "        top_k_idxs = np.argpartition(similarities, -self.k * 2)[-self.k * 2:]\n",
    "        top_k_sorted_idxs_dense = top_k_idxs[np.argsort(similarities[top_k_idxs])][::-1]\n",
    "        \n",
    "        # sparse: retrieve most similar documents\n",
    "        doc_scores = self.knowledge_base['index_sparse'].get_scores(query_tokens)\n",
    "        top_k_idxs = np.argpartition(doc_scores, -self.k * 2)[-self.k * 2:]\n",
    "        top_k_sorted_idxs_sparse = top_k_idxs[np.argsort(doc_scores[top_k_idxs])][::-1]\n",
    "\n",
    "        # hybrid score calculation\n",
    "        hybrid_table = {}\n",
    "        for idx, doc_id in enumerate(top_k_sorted_idxs_dense):\n",
    "            if doc_id not in hybrid_table:\n",
    "                hybrid_table[doc_id] = 0\n",
    "            if similarities[doc_id] > self.dense_search_min_similarity:\n",
    "                hybrid_table[doc_id] += self.alfa * (1 / (idx + 1))\n",
    "\n",
    "        for idx, doc_id in enumerate(top_k_sorted_idxs_sparse):\n",
    "            if doc_id not in hybrid_table:\n",
    "                hybrid_table[doc_id] = 0\n",
    "            if doc_scores[doc_id] > self.sparse_search_min_similarity:\n",
    "                hybrid_table[doc_id] += (1 - self.alfa) * (1 / (idx + 1))\n",
    "\n",
    "        hybrid_scored = sorted(hybrid_table.items(), key=lambda item: item[1], reverse = True)[:self.k]\n",
    "        top_k_sorted_idxs_hybrid, hybrid_scores = list(map(list, zip(*hybrid_scored)))\n",
    "\n",
    "        top_k_documents = self.knowledge_base['documents'].iloc[top_k_sorted_idxs_hybrid]\n",
    "    \n",
    "        return top_k_documents, hybrid_scores\n",
    "\n",
    "    def _construct_prompt(self, query_text, documents_text):\n",
    "        prompt = textwrap.dedent(\n",
    "            '''\\\n",
    "            <s>[INST]Use only the below-given KNOWLEDGE and not prior knowledge to provide an accurate, helpful, concise, and clear answer to the QUERY below.\n",
    "            Avoid copying word-for-word from the KNOWLEDGE and try to use your own words when possible.\n",
    "    \n",
    "            KNOWLEDGE:\n",
    "            {texts}\n",
    "    \n",
    "            Answer the QUERY using only the provided KNOWLEDGE. Don't provide notes, comments, or explanations.\n",
    "            After the answer, write the last line starting with \"Reference ids only: \" followed only by a list of the [id] of each reference article containing information needed to answer the query, for example \"[6], [11], [13]\", that's all.\n",
    "            If none of the articles from KNOWLEDGE contains information needed to provide a precise answer, or if you are not 100 % sure, reply with string: \"DONT_KNOW\".\n",
    "\n",
    "            QUERY: \"{query_text}\"\n",
    "            ANSWER:[/INST]\n",
    "            '''\n",
    "        ).format(\n",
    "            query_text = query_text,\n",
    "            texts = '\\n\\n'.join(['Article [{}]: \"\"\"\\n{}\\n\"\"\"'.format(idx, text) for idx, text in documents_text.reset_index(drop=True).items()])\n",
    "        )\n",
    "    \n",
    "        return prompt\n",
    "\n",
    "    def process_query(self, query):\n",
    "        documents, similarities = self._retriever(query)\n",
    "        prompt = self._construct_prompt(query, documents.content)\n",
    "        answer = self.client.execute_prompt(prompt)\n",
    "\n",
    "        answer_and_refs = answer.split('Reference ids only:')\n",
    "        answer = answer_and_refs[0].strip()\n",
    "\n",
    "        llm_references = []\n",
    "        used_ids = []\n",
    "        if len(answer_and_refs) > 1:\n",
    "            for ref_id in re.findall(r'\\[(\\d+)\\]', answer_and_refs[1]):\n",
    "                if ref_id not in used_ids:\n",
    "                    used_ids.append(ref_id)\n",
    "                    doc = documents.iloc[int(ref_id)]\n",
    "                    llm_references.append((doc.title, doc.url, similarities[int(ref_id)]))\n",
    "\n",
    "            llm_references = sorted(llm_references, key=lambda x: x[2], reverse=True)\n",
    "\n",
    "        if ('DONT_KNOW' in answer) or (len(llm_references) == 0):\n",
    "            answer = 'I\\'m sorry, I don\\'t know answer to your query.'\n",
    "            llm_references = []\n",
    "\n",
    "        return answer, llm_references"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "634a4151-17fd-4cbf-ad0d-3379b381f869",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG: \"In 2016, Socialbakers had 350 employees.\"\n",
      "     [ref: [('Emplifi', 'https://en.wikipedia.org/wiki/Emplifi', 1.0)]]\n",
      "RAG: \"Jan Rus is a Research Team Lead who is currently working at Emplifi.\"\n",
      "     [ref: [('Emplifi', 'https://en.wikipedia.org/wiki/Emplifi', 1.0)]]\n",
      "RAG: \"Jan Rus is currently working at Emplifi.\"\n",
      "     [ref: [('Emplifi', 'https://en.wikipedia.org/wiki/Emplifi', 0.6666666666666666)]]\n",
      "RAG: \"I'm sorry, I don't know answer to your query.\"\n",
      "     [ref: []]\n",
      "RAG: \"I'm sorry, I don't know answer to your query.\"\n",
      "     [ref: []]\n",
      "RAG: \"I'm sorry, I don't know answer to your query.\"\n",
      "     [ref: []]\n",
      "RAG: \"Yes, Russia is bigger than Belarus.\"\n",
      "     [ref: [('Belarus', 'https://en.wikipedia.org/wiki/Belarus', 0.2916666666666667), ('Russia', 'https://en.wikipedia.org/wiki/Russia', 0.16346153846153846)]]\n",
      "RAG: \"Russia is bigger than Belarus.\"\n",
      "     [ref: [('Belarus', 'https://en.wikipedia.org/wiki/Belarus', 0.35), ('Russia', 'https://en.wikipedia.org/wiki/Russia', 0.20833333333333331)]]\n"
     ]
    }
   ],
   "source": [
    "advanced_rag = AdvancedRAGQA()\n",
    "advanced_rag.load_knowledge_base('../data/wikipedia_kb/')\n",
    "\n",
    "query1 = 'How many employees did Socialbakers have in 2016?'\n",
    "query2 = 'Who is Jan Rus?'\n",
    "query3 = 'Where is Jan Rus working now?'\n",
    "query4 = 'What is a dog?'\n",
    "query5 = 'Do dogs love social media?'\n",
    "query6 = 'Hi!'\n",
    "query7 = 'Is Russia bigger than Belarus?'\n",
    "query8 = 'Which country is bigger, Russia or Belarus?'\n",
    "\n",
    "answer1, reference1 = advanced_rag.process_query(query1)\n",
    "answer2, reference2 = advanced_rag.process_query(query2)\n",
    "answer3, reference3 = advanced_rag.process_query(query3)\n",
    "answer4, reference4 = advanced_rag.process_query(query4)\n",
    "answer5, reference5 = advanced_rag.process_query(query5)\n",
    "answer6, reference6 = advanced_rag.process_query(query6)\n",
    "answer7, reference7 = advanced_rag.process_query(query7)\n",
    "answer8, reference8 = advanced_rag.process_query(query8)\n",
    "\n",
    "print(f'RAG: \"{answer1}\"\\n     [ref: {reference1}]')\n",
    "print(f'RAG: \"{answer2}\"\\n     [ref: {reference2}]')\n",
    "print(f'RAG: \"{answer3}\"\\n     [ref: {reference3}]')\n",
    "print(f'RAG: \"{answer4}\"\\n     [ref: {reference4}]')\n",
    "print(f'RAG: \"{answer5}\"\\n     [ref: {reference5}]')\n",
    "print(f'RAG: \"{answer6}\"\\n     [ref: {reference6}]')\n",
    "print(f'RAG: \"{answer7}\"\\n     [ref: {reference7}]')\n",
    "print(f'RAG: \"{answer8}\"\\n     [ref: {reference8}]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d98e2145-547e-40c1-a024-740a20bd49db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
