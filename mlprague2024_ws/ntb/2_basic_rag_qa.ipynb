{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06ca8260-b16d-41fc-9e29-28baf966a95d",
   "metadata": {},
   "source": [
    "### Install necessary dependencies\n",
    "If you need to uninstall the dependencies, you can run e.g. `!pip uninstall -r ../requirements.txt -y` in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4cf5b2ed-8883-42ed-8a99-1a5227bfd713",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m pip install -r ../requirements.txt --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03cd97d7-bf64-4c9b-8a71-ba7079f34538",
   "metadata": {},
   "source": [
    "### Make Python scripts accessible\n",
    "There is a couple of Python scripts in the `/src` directory. We can make them accessible by adding the directory to the path ENV variable. We insert the path to position 1 to make it the first path scanned for the required modules to not confuse our scripts with scripts with the same name but in unrelated locations. This change to ENV is temporary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "446ff7e7-293b-4963-bfe2-ba4986742894",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "path_to_src = os.path.abspath('../src')\n",
    "\n",
    "if path_to_src not in sys.path:\n",
    "    sys.path.insert(1, path_to_src)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea190c7-1876-4448-90bf-393a40809146",
   "metadata": {},
   "source": [
    "### Import dependencies\n",
    "\n",
    "Just common dependencies except the aws_cli - that is our own script for accessing Amazon Bedrock service. The Bedrock is a home of the models we are going to use for text embedding and text generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c4b4571-395d-42ea-9c25-2683520b35c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import textwrap\n",
    "\n",
    "from src.aws_cli import Client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62a0447d-ba45-4636-8f27-3d04869dcc30",
   "metadata": {},
   "source": [
    "### Naive RAG QA\n",
    "The solution from the previous notebook has been implemented in the form of a class. That's the only difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1c33078-dbce-459e-b90d-151c2a6a85b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveRAGQA:\n",
    "    def __init__(self):\n",
    "        self.client = Client()\n",
    "\n",
    "    def load_knowledge_base(self, dir_path):\n",
    "        # read all documents\n",
    "        documents_list = []\n",
    "        for file_name in os.listdir(dir_path):\n",
    "            if file_name.endswith('.json'):\n",
    "                documents_list.append(pd.read_json(os.path.join(dir_path, file_name)))\n",
    "\n",
    "        # build database\n",
    "        documents = pd.concat(documents_list, ignore_index=True)\n",
    "\n",
    "        # build index\n",
    "        embeddings = documents['content'].apply(self.client.embed_text)\n",
    "\n",
    "        # store knowledge base\n",
    "        self.knowledge_base = {'documents': documents, 'index': np.stack(embeddings, axis=0)}\n",
    "\n",
    "    def _retriever(self, query):\n",
    "        # embed user query\n",
    "        query_embedding = self.client.embed_text(query)  # use the same embedding that was used for the knowledge base\n",
    "\n",
    "        # retrieve most similar document\n",
    "        similarities = np.dot(self.knowledge_base['index'], query_embedding)\n",
    "        most_similar_idx = np.argmax(similarities)\n",
    "        document = self.knowledge_base['documents'].iloc[most_similar_idx]\n",
    "\n",
    "        return document, similarities[most_similar_idx]\n",
    "\n",
    "    def _construct_prompt(self, query_text, document_text):\n",
    "        prompt = textwrap.dedent(\n",
    "            f'''\\\n",
    "            <s>[INST]Use only the below-given KNOWLEDGE and not prior knowledge to provide an accurate, helpful, concise, and clear answer to the QUERY below.\n",
    "            Avoid copying word-for-word from the KNOWLEDGE and try to use your own words when possible.\n",
    "\n",
    "            KNOWLEDGE:\n",
    "            \"{document_text}\"\n",
    "\n",
    "            Answer the QUERY using the provided KNOWLEDGE. Do not provide notes, comments, or explanations.\n",
    "\n",
    "            QUERY: \"{query_text}\"\n",
    "            ANSWER:[/INST]\n",
    "            '''\n",
    "        )\n",
    "\n",
    "        return prompt\n",
    "\n",
    "    def process_query(self, query):\n",
    "        document, similarity = self._retriever(query)\n",
    "        prompt = self._construct_prompt(query, document.content)\n",
    "        answer = self.client.execute_prompt(prompt)\n",
    "\n",
    "        return answer, document.url"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "219ab93c",
   "metadata": {},
   "source": [
    "Create the naive RAG, load the Wikipedia knowledge base, and test it on a couple of examples from the previous notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e72f13bd-0700-42ff-a30b-d0bc6ebc96d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG: \"In 2016, Socialbakers had 350 employees.\"\n",
      "     [ref: https://en.wikipedia.org/wiki/Emplifi]\n"
     ]
    }
   ],
   "source": [
    "naive_rag = NaiveRAGQA()\n",
    "naive_rag.load_knowledge_base('../data/wikipedia_kb/')\n",
    "\n",
    "query = 'How many employees did Socialbakers have in 2016?'\n",
    "answer, reference = naive_rag.process_query(query)\n",
    "\n",
    "print(f'RAG: \"{answer}\"\\n     [ref: {reference}]')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00049a46-9915-4088-a33e-5a0828e70a6e",
   "metadata": {},
   "source": [
    "Let's check how well it works when the question is about something very specific, such as names. We added *Jan Rus* to the Emplifi Wikipedia page, but it's just one mention in a document that is mostly about the Emplifi company and there are another couple of documents with content containing a lot of *rus* tokens. Let's see if the naive RAG can find him."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e9341e4b-28e2-4e77-ba8a-ee27ee19c4e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG: \"Based on the provided KNOWLEDGE, there is no information about a person named \"Jan Rus.\" It is possible that there may be a misunderstanding or typo in the query, as \"Rus\" refers to the first East Slavic state, Kievan Rus', which arose in the 9th century and adopted Orthodox Christianity from the Byzantine Empire in 988. However, there is no mention of any individual named \"Jan\" in relation to Kievan Rus' or any other part of Russian history in the given KNOWLEDGE.\"\n",
      "     [ref: https://en.wikipedia.org/wiki/Russia]\n",
      "RAG: \"Based on the provided KNOWLEDGE, there is no information about Jan Rus or their current employment status. Therefore, I cannot provide an answer to this QUERY.\"\n",
      "     [ref: https://en.wikipedia.org/wiki/Belarus]\n"
     ]
    }
   ],
   "source": [
    "query1 = 'Who is Jan Rus?'\n",
    "query2 = 'Where is Jan Rus working now?'\n",
    "\n",
    "answer1, reference1 = naive_rag.process_query(query1)\n",
    "answer2, reference2 = naive_rag.process_query(query2)\n",
    "\n",
    "print(f'RAG: \"{answer1}\"\\n     [ref: {reference1}]')\n",
    "print(f'RAG: \"{answer2}\"\\n     [ref: {reference2}]')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbbc1756-bd3f-43df-9088-31bdc80cdc71",
   "metadata": {},
   "source": [
    "### Fixed-size chunking\n",
    "The single mention of the name Jan Rus vanished in the long document about a seemingly unrelated topic - Emplifi.\n",
    "Let's try to improve the RAG performance with chunking! The chunking may help because the resulting documents, the chunks,\n",
    "will be much shorter and the name Jan Rus will be more significant in the resulting embeddings. Chunking of the text just into fixed-size chunks could be sufficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d0de5754-c015-4747-a698-850d6b8c2a15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emplifi is an American private company headquartered in Columbus, Ohio. It develops and marketscustomer experience systems. The business was founded in 2020 after social media analytics company Socialbakers was acquiredby customer experience systems business Astute. The combined entity changed its name to Emplifi.\n",
      "['Emplifi is an American private company headquartered in Columbus, Ohio', ' Columbus, Ohio. It develops and marketscustomer experience systems. T', 'ence systems. The business was founded in 2020 after social media anal', 'cial media analytics company Socialbakers was acquiredby customer expe', 'y customer experience systems business Astute. The combined entity cha', 'ined entity changed its name to Emplifi.']\n"
     ]
    }
   ],
   "source": [
    "# some text, just for a quick experiment\n",
    "chunking_test_doc = '''\\\n",
    "Emplifi is an American private company headquartered in Columbus, Ohio. It develops and markets\\\n",
    "customer experience systems. The business was founded in 2020 after social media analytics company Socialbakers was acquired\\\n",
    "by customer experience systems business Astute. The combined entity changed its name to Emplifi.\\\n",
    "'''\n",
    "\n",
    "print(chunking_test_doc)\n",
    "\n",
    "# we can start with chunks having 70 characters\n",
    "# and 15 characters overlap between neighboring chunks\n",
    "chunk_size = 70\n",
    "chunk_overlap = 15\n",
    "\n",
    "def str2chunks(text, chunk_size, chunk_overlap):\n",
    "    # ensure the chunking settings make sense\n",
    "    chunk_size = abs(chunk_size)\n",
    "    chunk_overlap = abs(chunk_overlap)\n",
    "\n",
    "    if (chunk_size - chunk_overlap) < 1:\n",
    "        raise Exception('The chunk_size needs to be larger than chunk_overlap')\n",
    "\n",
    "    return [text[a:a + chunk_size] for a in range(0, len(text), chunk_size - chunk_overlap)]\n",
    "\n",
    "print(str2chunks(chunking_test_doc, chunk_size, chunk_overlap))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc3802a2-93a4-406d-a998-e648d57edb7b",
   "metadata": {},
   "source": [
    "Rewrite the `load_knowledge_base` function to start splitting loaded documents into chunks before they are vectorized and stored in the knowledge base. Rebuild the knowledge base and ensure its content looks as intended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ff9abace-19bc-4ff3-beaa-e63567ff84e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://en.wikipedia.org/wiki/Russia</td>\n",
       "      <td>Russia</td>\n",
       "      <td>Russia, or the Russian Federation, is a countr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://en.wikipedia.org/wiki/Russia</td>\n",
       "      <td>Russia</td>\n",
       "      <td>g Eastern Europe and North Asia. Russia is the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://en.wikipedia.org/wiki/Russia</td>\n",
       "      <td>Russia</td>\n",
       "      <td>country in the world by area, extending across...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://en.wikipedia.org/wiki/Russia</td>\n",
       "      <td>Russia</td>\n",
       "      <td>ime zones and sharing land borders with fourte...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://en.wikipedia.org/wiki/Russia</td>\n",
       "      <td>Russia</td>\n",
       "      <td>ies. It is the world's ninth-most populous cou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>https://en.wikipedia.org/wiki/Russia</td>\n",
       "      <td>Russia</td>\n",
       "      <td>Europe's most populous country. The country's ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>https://en.wikipedia.org/wiki/Russia</td>\n",
       "      <td>Russia</td>\n",
       "      <td>s well as its largest city is Moscow. Saint Pe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>https://en.wikipedia.org/wiki/Russia</td>\n",
       "      <td>Russia</td>\n",
       "      <td>is Russia's second-largest city and cultural c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>https://en.wikipedia.org/wiki/Russia</td>\n",
       "      <td>Russia</td>\n",
       "      <td>ther major cities in the country include Novos...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>https://en.wikipedia.org/wiki/Russia</td>\n",
       "      <td>Russia</td>\n",
       "      <td>ekaterinburg, Nizhny Novgorod, Chelyabinsk, Kr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    url   title  \\\n",
       "0  https://en.wikipedia.org/wiki/Russia  Russia   \n",
       "1  https://en.wikipedia.org/wiki/Russia  Russia   \n",
       "2  https://en.wikipedia.org/wiki/Russia  Russia   \n",
       "3  https://en.wikipedia.org/wiki/Russia  Russia   \n",
       "4  https://en.wikipedia.org/wiki/Russia  Russia   \n",
       "5  https://en.wikipedia.org/wiki/Russia  Russia   \n",
       "6  https://en.wikipedia.org/wiki/Russia  Russia   \n",
       "7  https://en.wikipedia.org/wiki/Russia  Russia   \n",
       "8  https://en.wikipedia.org/wiki/Russia  Russia   \n",
       "9  https://en.wikipedia.org/wiki/Russia  Russia   \n",
       "\n",
       "                                             content  \n",
       "0  Russia, or the Russian Federation, is a countr...  \n",
       "1  g Eastern Europe and North Asia. Russia is the...  \n",
       "2  country in the world by area, extending across...  \n",
       "3  ime zones and sharing land borders with fourte...  \n",
       "4  ies. It is the world's ninth-most populous cou...  \n",
       "5  Europe's most populous country. The country's ...  \n",
       "6  s well as its largest city is Moscow. Saint Pe...  \n",
       "7  is Russia's second-largest city and cultural c...  \n",
       "8  ther major cities in the country include Novos...  \n",
       "9  ekaterinburg, Nizhny Novgorod, Chelyabinsk, Kr...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def load_knowledge_base(self, dir_path):\n",
    "    # read all documents\n",
    "    documents_list = []\n",
    "    for file_name in os.listdir(dir_path):\n",
    "        if file_name.endswith('.json'):\n",
    "            documents_list.append(pd.read_json(os.path.join(dir_path, file_name)))\n",
    "\n",
    "    # build database\n",
    "    documents = pd.concat(documents_list, ignore_index=True)\n",
    "\n",
    "    # split documents into chunks\n",
    "    documents['chunks'] = documents['content'].apply(str2chunks, chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    documents = documents.explode('chunks').reset_index(drop=True)\n",
    "    documents = documents.drop(columns=['content'])\n",
    "    documents = documents.rename(columns={'chunks': 'content'})\n",
    "\n",
    "    # build index\n",
    "    embeddings = documents['content'].apply(self.client.embed_text)\n",
    "\n",
    "    # store knowledge base\n",
    "    self.knowledge_base = {'documents': documents, 'index': np.stack(embeddings, axis=0)}\n",
    "\n",
    "naive_rag.load_knowledge_base = load_knowledge_base\n",
    "naive_rag.load_knowledge_base(naive_rag, '../data/wikipedia_kb/')\n",
    "\n",
    "display(naive_rag.knowledge_base['documents'].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "734142a7-ce4c-4c37-be76-a3850aa9de43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG: \"Jan Rus is a Research Team Lead.\n",
      "\n",
      "(Note: The provided KNOWLEDGE was incomplete and cut off, so I used the available information to answer the QUERY concisely and clearly.)\"\n",
      "     [ref: https://en.wikipedia.org/wiki/Emplifi]\n",
      "RAG: \"Based on the provided knowledge, Jan Rus is currently working as a Research Team Lead. However, the specific location or organization where he is working is not mentioned in the given text.\"\n",
      "     [ref: https://en.wikipedia.org/wiki/Emplifi]\n"
     ]
    }
   ],
   "source": [
    "query1 = 'Who is Jan Rus?'\n",
    "query2 = 'Where is Jan Rus working now?'\n",
    "\n",
    "answer1, reference1 = naive_rag.process_query(query1)\n",
    "answer2, reference2 = naive_rag.process_query(query2)\n",
    "\n",
    "print(f'RAG: \"{answer1}\"\\n     [ref: {reference1}]')\n",
    "print(f'RAG: \"{answer2}\"\\n     [ref: {reference2}]')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "968a4d20-789b-4e11-a34c-3c8aebf609c5",
   "metadata": {},
   "source": [
    "Better! The RAG can find the name now but not the organization where he works. Why? Let's peek into the knowledge base and look for Jan Rus ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "43dfd049-cc65-42c2-ac5f-1e206b182091",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>https://en.wikipedia.org/wiki/Emplifi</td>\n",
       "      <td>Emplifi</td>\n",
       "      <td>nding over the years. Jan Rus is a Research Te...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      url    title  \\\n",
       "84  https://en.wikipedia.org/wiki/Emplifi  Emplifi   \n",
       "\n",
       "                                              content  \n",
       "84  nding over the years. Jan Rus is a Research Te...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"nding over the years. Jan Rus is a Research Team Lead, he is working a\"\n"
     ]
    }
   ],
   "source": [
    "all_chunks = naive_rag.knowledge_base['documents'][naive_rag.knowledge_base['documents']['content'].str.contains('Jan Rus')]\n",
    "display(all_chunks)\n",
    "print(f'\"{all_chunks.iloc[0].content}\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c29e178a-4e95-4128-bef6-76241666c113",
   "metadata": {},
   "source": [
    "It seems the chunks are too short and the information in them can miss context. Let's make them bigger - change the `chunk_size` to 250 and `chunk_overlap` to 50 and re-run the experiment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f9bd56-39f3-4dad-8263-c5698502af44",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf4c81b-1015-4c52-84ec-92067c26a33d",
   "metadata": {},
   "source": [
    "We expected it will end up better. Let's see what is in the top 6 documents most relevant to the query. Maybe the right document is quite similar just not the most similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "35eb057a-f6c1-4f6e-8ace-f69ea5fec9c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "      <th>similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>https://en.wikipedia.org/wiki/Emplifi</td>\n",
       "      <td>Emplifi</td>\n",
       "      <td>nding over the years. Jan Rus is a Research Te...</td>\n",
       "      <td>139.141512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>https://en.wikipedia.org/wiki/Russia</td>\n",
       "      <td>Russia</td>\n",
       "      <td>ekaterinburg, Nizhny Novgorod, Chelyabinsk, Kr...</td>\n",
       "      <td>119.202930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://en.wikipedia.org/wiki/Russia</td>\n",
       "      <td>Russia</td>\n",
       "      <td>Russia, or the Russian Federation, is a countr...</td>\n",
       "      <td>110.334886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>https://en.wikipedia.org/wiki/Belarus</td>\n",
       "      <td>Belarus</td>\n",
       "      <td>ia to the east and northeast, Ukraine to the s...</td>\n",
       "      <td>107.426976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>https://en.wikipedia.org/wiki/Belarus</td>\n",
       "      <td>Belarus</td>\n",
       "      <td>itics until well into the 1970s, overseeing Be...</td>\n",
       "      <td>105.321153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>https://en.wikipedia.org/wiki/Russia</td>\n",
       "      <td>Russia</td>\n",
       "      <td>, Kazan, Krasnodar and Rostov-on-Don.\\n       ...</td>\n",
       "      <td>104.819989</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       url    title  \\\n",
       "84   https://en.wikipedia.org/wiki/Emplifi  Emplifi   \n",
       "9     https://en.wikipedia.org/wiki/Russia   Russia   \n",
       "0     https://en.wikipedia.org/wiki/Russia   Russia   \n",
       "122  https://en.wikipedia.org/wiki/Belarus  Belarus   \n",
       "153  https://en.wikipedia.org/wiki/Belarus  Belarus   \n",
       "10    https://en.wikipedia.org/wiki/Russia   Russia   \n",
       "\n",
       "                                               content  similarity  \n",
       "84   nding over the years. Jan Rus is a Research Te...  139.141512  \n",
       "9    ekaterinburg, Nizhny Novgorod, Chelyabinsk, Kr...  119.202930  \n",
       "0    Russia, or the Russian Federation, is a countr...  110.334886  \n",
       "122  ia to the east and northeast, Ukraine to the s...  107.426976  \n",
       "153  itics until well into the 1970s, overseeing Be...  105.321153  \n",
       "10   , Kazan, Krasnodar and Rostov-on-Don.\\n       ...  104.819989  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# embedding of \"Where is Jan Rus working now?\"\n",
    "query_embedding = naive_rag.client.embed_text(query2)\n",
    "\n",
    "# retrieve 6 most similar documents\n",
    "similarities = np.dot(naive_rag.knowledge_base['index'], query_embedding)\n",
    "most_similar_idxs = pd.Series(similarities).sort_values(ascending=False).index.values[0:6]\n",
    "documents = pd.DataFrame(naive_rag.knowledge_base['documents'].iloc[most_similar_idxs])\n",
    "documents['similarity'] = similarities[most_similar_idxs]\n",
    "\n",
    "display(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7966ea73-d6e5-45ba-bc88-4f5a03af4f9f",
   "metadata": {},
   "source": [
    "So, the chunking is an improvement and we are quite close to retrieving the right chunk/knowledge for the second query. Let's create a copy of the RAG class and add the chunking because it helps. Then, we can try another approach ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2311bd87-96ab-4196-a587-1f692e01b85b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicRAGQA:\n",
    "    def __init__(self, chunk_size=250, chunk_overlap=50):\n",
    "        self.client = Client()\n",
    "\n",
    "        # ensure the chunking settings make sense\n",
    "        chunk_size = abs(chunk_size)\n",
    "        chunk_overlap = abs(chunk_overlap)\n",
    "\n",
    "        if (chunk_size - chunk_overlap) < 1:\n",
    "            raise Exception('The chunk_size needs to be larger than chunk_overlap')\n",
    "\n",
    "        self.chunk_size = chunk_size\n",
    "        self.chunk_overlap = chunk_overlap\n",
    "\n",
    "\n",
    "    def load_knowledge_base(self, dir_path):\n",
    "        # read all documents\n",
    "        documents_list = []\n",
    "        for file_name in os.listdir(dir_path):\n",
    "            if file_name.endswith('.json'):\n",
    "                documents_list.append(pd.read_json(os.path.join(dir_path, file_name)))\n",
    "\n",
    "        # build database\n",
    "        documents = pd.concat(documents_list, ignore_index=True)\n",
    "\n",
    "        # split documents into chunks\n",
    "        documents['chunks'] = documents['content'].apply(self._str2chunks)\n",
    "        documents = documents.explode('chunks').reset_index(drop=True)\n",
    "        documents = documents.drop(columns=['content'])\n",
    "        documents = documents.rename(columns={'chunks': 'content'})\n",
    "\n",
    "        # build index\n",
    "        embeddings = documents['content'].apply(self.client.embed_text)\n",
    "\n",
    "        # store knowledge base\n",
    "        self.knowledge_base = {'documents': documents, 'index': np.stack(embeddings, axis=0)}\n",
    "\n",
    "    def _str2chunks(self, text):\n",
    "        return [text[a:a + self.chunk_size] for a in range(0, len(text), self.chunk_size - self.chunk_overlap)]\n",
    "\n",
    "    def _retriever(self, query):\n",
    "        # embed user query\n",
    "        query_embedding = self.client.embed_text(query)  # use the same embedding that was used for the knowledge base\n",
    "\n",
    "        # retrieve most similar document\n",
    "        similarities = np.dot(self.knowledge_base['index'], query_embedding)\n",
    "        most_similar_idx = np.argmax(similarities)\n",
    "        document = self.knowledge_base['documents'].iloc[most_similar_idx]\n",
    "\n",
    "        return document, similarities[most_similar_idx]\n",
    "\n",
    "    def _construct_prompt(self, query_text, document_text):\n",
    "        prompt = textwrap.dedent(\n",
    "            f'''\\\n",
    "            <s>[INST]Use only the below-given KNOWLEDGE and not prior knowledge to provide an accurate, helpful, concise, and clear answer to the QUERY below.\n",
    "            Avoid copying word-for-word from the KNOWLEDGE and try to use your own words when possible.\n",
    "\n",
    "            KNOWLEDGE:\n",
    "            \"{document_text}\"\n",
    "\n",
    "            Answer the QUERY using the provided KNOWLEDGE. Do not provide notes, comments, or explanations.\n",
    "\n",
    "            QUERY: \"{query_text}\"\n",
    "            ANSWER:[/INST]\n",
    "            '''\n",
    "        )\n",
    "\n",
    "        return prompt\n",
    "\n",
    "    def process_query(self, query):\n",
    "        document, similarity = self._retriever(query)\n",
    "        prompt = self._construct_prompt(query, document.content)\n",
    "        answer = self.client.execute_prompt(prompt)\n",
    "\n",
    "        return answer, document.url"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16af5127-13e2-470b-93bd-d3761f702992",
   "metadata": {},
   "source": [
    "### Retrieve more chunks\n",
    "We can try to use KNN (k nearest neighbors) instead of the current NN (nearest neighbor) to retrieve not 1 but k chunks. We can use all the chunks to augment the prompt sent to the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "07be7ef5-dc6d-472b-bddf-096a1a1e2585",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicRAGQA:\n",
    "    def __init__(self, chunk_size=250, chunk_overlap=50, k=8):\n",
    "        self.client = Client()\n",
    "\n",
    "        # ensure the chunking settings make sense\n",
    "        chunk_size = abs(chunk_size)\n",
    "        chunk_overlap = abs(chunk_overlap)\n",
    "\n",
    "        if (chunk_size - chunk_overlap) < 1:\n",
    "            raise Exception('The chunk_size needs to be larger than chunk_overlap')\n",
    "\n",
    "        k = abs(k)\n",
    "\n",
    "        if not 0 < k <= 20:\n",
    "            raise Exception('The k needs to be between 1 and 20')\n",
    "\n",
    "        self.chunk_size = chunk_size\n",
    "        self.chunk_overlap = chunk_overlap\n",
    "        self.k = k\n",
    "\n",
    "\n",
    "    def load_knowledge_base(self, dir_path):\n",
    "        # read all documents\n",
    "        documents_list = []\n",
    "        for file_name in os.listdir(dir_path):\n",
    "            if file_name.endswith('.json'):\n",
    "                documents_list.append(pd.read_json(os.path.join(dir_path, file_name)))\n",
    "\n",
    "        # build database\n",
    "        documents = pd.concat(documents_list, ignore_index=True)\n",
    "\n",
    "        # split documents into chunks\n",
    "        documents['chunks'] = documents['content'].apply(self._str2chunks)\n",
    "        documents = documents.explode('chunks').reset_index(drop=True)\n",
    "        documents = documents.drop(columns=['content'])\n",
    "        documents = documents.rename(columns={'chunks': 'content'})\n",
    "\n",
    "        # build index\n",
    "        embeddings = documents['content'].apply(self.client.embed_text)\n",
    "\n",
    "        # store knowledge base\n",
    "        self.knowledge_base = {'documents': documents, 'index': np.stack(embeddings, axis=0)}\n",
    "\n",
    "    def _str2chunks(self, text):\n",
    "        return [text[a:a + self.chunk_size] for a in range(0, len(text), self.chunk_size - self.chunk_overlap)]\n",
    "\n",
    "    def _retriever(self, query):\n",
    "        # embed user query\n",
    "        query_embedding = self.client.embed_text(query)  # use the same embedding that was used for the knowledge base\n",
    "\n",
    "        # retrieve most similar document\n",
    "        similarities = np.dot(self.knowledge_base['index'], query_embedding)\n",
    "        top_k_idxs = np.argpartition(similarities, -self.k)[-self.k:]\n",
    "        top_k_sorted_idxs = top_k_idxs[np.argsort(similarities[top_k_idxs])][::-1]\n",
    "        top_k_documents = self.knowledge_base['documents'].iloc[top_k_sorted_idxs]\n",
    "\n",
    "        return top_k_documents, similarities[top_k_sorted_idxs]\n",
    "\n",
    "    def _construct_prompt(self, query_text, documents_text):\n",
    "        prompt = textwrap.dedent(\n",
    "            '''\\\n",
    "            <s>[INST]Use only the below-given KNOWLEDGE and not prior knowledge to provide an accurate, helpful, concise, and clear answer to the QUERY below.\n",
    "            Avoid copying word-for-word from the KNOWLEDGE and try to use your own words when possible.\n",
    "\n",
    "            KNOWLEDGE:\n",
    "            {texts}\n",
    "\n",
    "            Answer the QUERY using the provided KNOWLEDGE. Do not provide notes, comments, or explanations.\n",
    "            After the answer, write a paragraph starting with \"References: \" followed by the [id] of each reference article containing information needed to answer the query.\n",
    "\n",
    "\n",
    "            QUERY: \"{query_text}\"\n",
    "            ANSWER:[/INST]\n",
    "            '''\n",
    "        ).format(\n",
    "            query_text = query_text,\n",
    "            texts = '\\n\\n'.join(['Article [{}]: \"\"\"\\n{}\\n\"\"\"'.format(idx, text) for idx, text in documents_text.reset_index(drop=True).items()])\n",
    "        )\n",
    "\n",
    "        return prompt\n",
    "\n",
    "    def process_query(self, query):\n",
    "        documents, similarities = self._retriever(query)\n",
    "        prompt = self._construct_prompt(query, documents.content)\n",
    "        answer = self.client.execute_prompt(prompt)\n",
    "\n",
    "        answer_and_refs = answer.split('References:')\n",
    "        answer = answer_and_refs[0].strip()\n",
    "\n",
    "        llm_references = []\n",
    "        if len(answer_and_refs) > 1:\n",
    "            for ref_id in re.findall(r'\\[(\\d+)\\]', answer_and_refs[1]):\n",
    "                doc = documents.iloc[int(ref_id)]\n",
    "                llm_references.append((doc.title, doc.url, similarities[int(ref_id)]))\n",
    "\n",
    "            llm_references = sorted(llm_references, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        return answer, llm_references"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f193d15-ec46-490c-910a-d2d1a74bc697",
   "metadata": {},
   "source": [
    "Test the effect of retrieving more chunks on the easy example, just to see it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "32f953c9-3ec6-4aa0-8ce3-f0a4baf6833d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG: \"In 2016, Socialbakers had 350 employees.\"\n",
      "     [refs: [('Emplifi', 'https://en.wikipedia.org/wiki/Emplifi', 126.11330250122865), ('Emplifi', 'https://en.wikipedia.org/wiki/Emplifi', 82.78630937431362)]]\n"
     ]
    }
   ],
   "source": [
    "basic_rag = BasicRAGQA()\n",
    "basic_rag.load_knowledge_base('../data/wikipedia_kb/')\n",
    "\n",
    "query = 'How many employees did Socialbakers have in 2016?'\n",
    "answer, references = basic_rag.process_query(query)\n",
    "\n",
    "print(f'RAG: \"{answer}\"\\n     [refs: {references}]')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da7669f-24ad-46d3-9a36-697631dc3576",
   "metadata": {},
   "source": [
    "And it works on the broken example too!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c4e31ee6-acbd-42bc-bf43-ee25ef24e913",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG: \"Jan Rus is a Research Team Lead who is currently working at Emplifi.\"\n",
      "     [refs: [('Emplifi', 'https://en.wikipedia.org/wiki/Emplifi', 99.09258361662546)]]\n",
      "RAG: \"Jan Rus is currently working at Emplifi.\"\n",
      "     [refs: [('Emplifi', 'https://en.wikipedia.org/wiki/Emplifi', 86.93297576438992), ('Emplifi', 'https://en.wikipedia.org/wiki/Emplifi', 86.93297576438992)]]\n"
     ]
    }
   ],
   "source": [
    "query1 = 'Who is Jan Rus?'\n",
    "query2 = 'Where is Jan Rus working now?'\n",
    "\n",
    "answer1, references1 = basic_rag.process_query(query1)\n",
    "answer2, references2 = basic_rag.process_query(query2)\n",
    "\n",
    "print(f'RAG: \"{answer1}\"\\n     [refs: {references1}]')\n",
    "print(f'RAG: \"{answer2}\"\\n     [refs: {references2}]')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c6fcc9-c952-4b36-9c98-f1f1f855d5b9",
   "metadata": {},
   "source": [
    "But will this version of RAG QA work with questions aiming outside of the KB domain? The first query completely misses the KB, there are no relevant documents. But the second query is harder - it is possible to find a document that is relevant to the \"social media\" part of the question ad it can confuse the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "38c69bcf-9157-4f29-9c75-5f3070700d91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG: \"Query not answerable using provided knowledge.\"\n",
      "     [refs: [('Belarus', 'https://en.wikipedia.org/wiki/Belarus', 35.417392358065236), ('Belarus', 'https://en.wikipedia.org/wiki/Belarus', 22.147229189567007)]]\n",
      "RAG: \"Based on the provided knowledge, there is no information regarding dogs and their love for social media.\"\n",
      "     [refs: []]\n",
      "RAG: \"Hello! How can I assist you today?\"\n",
      "     [refs: [('Yuval Ben-Itzhak', 'https://en.wikipedia.org/wiki/Yuval_Ben-Itzhak', 20.971611589100025), ('Yuval Ben-Itzhak', 'https://en.wikipedia.org/wiki/Yuval_Ben-Itzhak', 19.230715491606723), ('Yuval Ben-Itzhak', 'https://en.wikipedia.org/wiki/Yuval_Ben-Itzhak', 19.100102402148362), ('Yuval Ben-Itzhak', 'https://en.wikipedia.org/wiki/Yuval_Ben-Itzhak', 18.50406482318339), ('Yuval Ben-Itzhak', 'https://en.wikipedia.org/wiki/Yuval_Ben-Itzhak', 15.78845807208484), ('Emplifi', 'https://en.wikipedia.org/wiki/Emplifi', 16.984539112920615), ('Belarus', 'https://en.wikipedia.org/wiki/Belarus', 20.858565615888182), ('Belarus', 'https://en.wikipedia.org/wiki/Belarus', 19.489341096829087)]]\n"
     ]
    }
   ],
   "source": [
    "query1 = 'What is a dog?'\n",
    "query2 = 'Do dogs love social media?'\n",
    "query3 = 'Hi!'\n",
    "\n",
    "answer1, references1 = basic_rag.process_query(query1)\n",
    "answer2, references2 = basic_rag.process_query(query2)\n",
    "answer3, references3 = basic_rag.process_query(query3)\n",
    "\n",
    "print(f'RAG: \"{answer1}\"\\n     [refs: {references1}]')\n",
    "print(f'RAG: \"{answer2}\"\\n     [refs: {references2}]')\n",
    "print(f'RAG: \"{answer3}\"\\n     [refs: {references3}]')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec71d9e8-1335-4ef9-a7c3-bb9a597481e7",
   "metadata": {},
   "source": [
    "### \"I don't know\" state\n",
    "There are cases when the RAG system doesn't have enough relevant context to respond to the user query reliably. In these cases, the RAG needs to have an option to say \"I don't know\" to further suppress potential hallucinations. Let's do some more prompt engineering ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e163bc14-0982-48af-ad04-ab7c7a0c85c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicRAGQA:\n",
    "    def __init__(self, chunk_size=250, chunk_overlap=50, k=8):\n",
    "        self.client = Client()\n",
    "\n",
    "        # ensure the chunking settings make sense\n",
    "        chunk_size = abs(chunk_size)\n",
    "        chunk_overlap = abs(chunk_overlap)\n",
    "\n",
    "        if (chunk_size - chunk_overlap) < 1:\n",
    "            raise Exception('The chunk_size needs to be larger than chunk_overlap')\n",
    "\n",
    "        k = abs(k)\n",
    "\n",
    "        if not 0 < k <= 20:\n",
    "            raise Exception('The k needs to be between 1 and 20')\n",
    "\n",
    "        self.chunk_size = chunk_size\n",
    "        self.chunk_overlap = chunk_overlap\n",
    "        self.k = k\n",
    "\n",
    "\n",
    "    def load_knowledge_base(self, dir_path):\n",
    "        # read all documents\n",
    "        documents_list = []\n",
    "        for file_name in os.listdir(dir_path):\n",
    "            if file_name.endswith('.json'):\n",
    "                documents_list.append(pd.read_json(os.path.join(dir_path, file_name)))\n",
    "\n",
    "        # build database\n",
    "        documents = pd.concat(documents_list, ignore_index=True)\n",
    "\n",
    "        # split documents into chunks\n",
    "        documents['chunks'] = documents['content'].apply(self._str2chunks)\n",
    "        documents = documents.explode('chunks').reset_index(drop=True)\n",
    "        documents = documents.drop(columns=['content'])\n",
    "        documents = documents.rename(columns={'chunks': 'content'})\n",
    "\n",
    "        # build index\n",
    "        embeddings = documents['content'].apply(self.client.embed_text)\n",
    "\n",
    "        # store knowledge base\n",
    "        self.knowledge_base = {'documents': documents, 'index': np.stack(embeddings, axis=0)}\n",
    "\n",
    "    def _str2chunks(self, text):\n",
    "        return [text[a:a + self.chunk_size] for a in range(0, len(text), self.chunk_size - self.chunk_overlap)]\n",
    "\n",
    "    def _retriever(self, query):\n",
    "        # embed user query\n",
    "        query_embedding = self.client.embed_text(query)  # use the same embedding that was used for the knowledge base\n",
    "\n",
    "        # retrieve most similar document\n",
    "        similarities = np.dot(self.knowledge_base['index'], query_embedding)\n",
    "        top_k_idxs = np.argpartition(similarities, -self.k)[-self.k:]\n",
    "        top_k_sorted_idxs = top_k_idxs[np.argsort(similarities[top_k_idxs])][::-1]\n",
    "        top_k_documents = self.knowledge_base['documents'].iloc[top_k_sorted_idxs]\n",
    "\n",
    "        return top_k_documents, similarities[top_k_sorted_idxs]\n",
    "\n",
    "    def _construct_prompt(self, query_text, documents_text):\n",
    "        prompt = textwrap.dedent(\n",
    "            '''\\\n",
    "            <s>[INST]Use only the below-given KNOWLEDGE and not prior knowledge to provide an accurate, helpful, concise, and clear answer to the QUERY below.\n",
    "            Avoid copying word-for-word from the KNOWLEDGE and try to use your own words when possible.\n",
    "\n",
    "            KNOWLEDGE:\n",
    "            {texts}\n",
    "\n",
    "            Answer the QUERY using only the provided KNOWLEDGE. Don't provide notes, comments, or explanations.\n",
    "            After the answer, write a paragraph starting with \"References: \" followed by the [id] of each reference article containing information needed to answer the query.\n",
    "            If none of the articles from KNOWLEDGE contains information needed to provide a precise answer, or if you are not 100 % sure, reply with string: \"DONT_KNOW\".\n",
    "\n",
    "            QUERY: \"{query_text}\"\n",
    "            ANSWER:[/INST]\n",
    "            '''\n",
    "        ).format(\n",
    "            query_text = query_text,\n",
    "            texts = '\\n\\n'.join(['Article [{}]: \"\"\"\\n{}\\n\"\"\"'.format(idx, text) for idx, text in documents_text.reset_index(drop=True).items()])\n",
    "        )\n",
    "\n",
    "        return prompt\n",
    "\n",
    "    def process_query(self, query):\n",
    "        documents, similarities = self._retriever(query)\n",
    "        prompt = self._construct_prompt(query, documents.content)\n",
    "        answer = self.client.execute_prompt(prompt)\n",
    "\n",
    "        answer_and_refs = answer.split('References:')\n",
    "        answer = answer_and_refs[0].strip()\n",
    "\n",
    "        llm_references = []\n",
    "        if len(answer_and_refs) > 1:\n",
    "            for ref_id in re.findall(r'\\[(\\d+)\\]', answer_and_refs[1]):\n",
    "                doc = documents.iloc[int(ref_id)]\n",
    "                llm_references.append((doc.title, doc.url, similarities[int(ref_id)]))\n",
    "\n",
    "            llm_references = sorted(llm_references, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        if ('DONT_KNOW' in answer) or (len(llm_references) == 0):\n",
    "            answer = 'I\\'m sorry, I don\\'t know answer to your query.'\n",
    "            llm_references = []\n",
    "\n",
    "        return answer, llm_references"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e5d22c5b-c973-43a7-adb6-b14a0c0ff631",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG: \"I'm sorry, I don't know answer to your query.\"\n",
      "     [refs: []]\n",
      "RAG: \"I'm sorry, I don't know answer to your query.\"\n",
      "     [refs: []]\n",
      "RAG: \"I'm sorry, I don't know answer to your query.\"\n",
      "     [refs: []]\n"
     ]
    }
   ],
   "source": [
    "basic_rag = BasicRAGQA()\n",
    "basic_rag.load_knowledge_base('../data/wikipedia_kb/')\n",
    "\n",
    "query1 = 'What is a dog?'\n",
    "query2 = 'Do dogs love social media?'\n",
    "query3 = 'Hi!'\n",
    "\n",
    "answer1, references1 = basic_rag.process_query(query1)\n",
    "answer2, references2 = basic_rag.process_query(query2)\n",
    "answer3, references3 = basic_rag.process_query(query3)\n",
    "\n",
    "print(f'RAG: \"{answer1}\"\\n     [refs: {references1}]')\n",
    "print(f'RAG: \"{answer2}\"\\n     [refs: {references2}]')\n",
    "print(f'RAG: \"{answer3}\"\\n     [refs: {references3}]')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef9c0711-808e-4dd4-baa2-ce0c8f0b96ec",
   "metadata": {},
   "source": [
    "Here we have the last example of inputs which can cause issues. Notice that the RAG replied correctly but without any reference to Russia. Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "be51ccdd-f172-410a-8e3b-33290cc07f31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG: \"Based on the provided knowledge, Russia is bigger than Belarus.\"\n",
      "     [refs: [('Belarus', 'https://en.wikipedia.org/wiki/Belarus', 205.95386592863792), ('Belarus', 'https://en.wikipedia.org/wiki/Belarus', 198.6775665542217)]]\n",
      "RAG: \"Based on the provided knowledge, Russia is bigger than Belarus.\"\n",
      "     [refs: [('Belarus', 'https://en.wikipedia.org/wiki/Belarus', 187.64516225245112), ('Belarus', 'https://en.wikipedia.org/wiki/Belarus', 185.5182271467039), ('Belarus', 'https://en.wikipedia.org/wiki/Belarus', 161.03730269652505)]]\n"
     ]
    }
   ],
   "source": [
    "query1 = 'Is Russia bigger than Belarus?'  # Elaborate\n",
    "query2 = 'Which country is bigger, Russia or Belarus?'  # Elaborate\n",
    "\n",
    "answer1, references1 = basic_rag.process_query(query1)\n",
    "answer2, references2 = basic_rag.process_query(query2)\n",
    "\n",
    "print(f'RAG: \"{answer1}\"\\n     [refs: {references1}]')\n",
    "print(f'RAG: \"{answer2}\"\\n     [refs: {references2}]')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd1b314c-8d2a-458d-ae19-66e3e7191ec9",
   "metadata": {},
   "source": [
    "This example is super confusing for the RAG because not only we can find relevant documents but they even contain part of the needed information. Unfortunately, the retrieval is not good in this case and the chunk about Russia that could help to correctly answer these questions with proper reasoning and references is not retrieved. Can you fix it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e215a2-8794-4f59-8aee-b5c1823008f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
